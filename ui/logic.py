"""
UI Logic - æ ¸å¿ƒä¸šåŠ¡é€»è¾‘

å°†å¤„ç†é€»è¾‘ä» UI å±‚åˆ†ç¦»ï¼Œæ”¯æŒå¤šç”¨æˆ·å¹¶å‘ã€‚
"""

import numpy as np
from typing import Any

from .state import ProcessingState, compute_image_hash
from .config import SEMANTIC_COLORS, SEMANTIC_BUCKETS

# Pipeline å»¶è¿Ÿå¯¼å…¥
_pipeline = None


def get_pipeline():
    """æ‡’åŠ è½½ Pipeline"""
    global _pipeline
    if _pipeline is None:
        from src.pipeline import load_pipeline
        _pipeline = load_pipeline()
    return _pipeline


def full_compute(
    state: ProcessingState,
    image: np.ndarray,
    traditional_smooth_method: str,
    traditional_k: int,
    use_diffusion: bool = False
) -> ProcessingState:
    """
    å®Œæ•´è®¡ç®—ï¼ˆéœ€è¦æ¨¡å‹æ¨ç†ï¼‰- Stage 1
    
    Args:
        state: ç”¨æˆ·ä¼šè¯çŠ¶æ€
        image: è¾“å…¥å›¾åƒ
        traditional_smooth_method: å¹³æ»‘æ–¹æ³•
        traditional_k: K å€¼
        use_diffusion: æ˜¯å¦å¯ç”¨ Diffusion
    
    Returns:
        æ›´æ–°åçš„çŠ¶æ€
    """
    if image is None:
        return state
    
    pipe = get_pipeline()
    img_hash = compute_image_hash(image)
    trad_params = (traditional_k, traditional_smooth_method, use_diffusion)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è®¡ç®—
    if state.image_hash == img_hash and state.trad_params == trad_params:
        return state  # ä½¿ç”¨ç¼“å­˜
    
    print("[Pipeline] æ‰§è¡Œå®Œæ•´è®¡ç®—...")
    print(f"[Pipeline] Diffusion æ¨¡å¼: {'å¯ç”¨' if use_diffusion else 'å…³é—­'}")
    
    # åˆ›å»ºæ–°çŠ¶æ€
    new_state = state.copy()
    
    # A. é¢„å¤„ç†
    ctx = pipe.preprocessor.process(image)
    
    # B. è¯­ä¹‰åˆ†æ
    seg_out = pipe.segmenter.predict(ctx.image_f32)
    face_mask = None
    if pipe.face_detector:
        face_mask = pipe.face_detector.detect(ctx.image_u8)
    
    # C. é£æ ¼å€™é€‰ç”Ÿæˆ
    ui_params = {
        "traditional_k": traditional_k,
        "traditional_smooth_method": traditional_smooth_method,
        "use_diffusion": use_diffusion
    }
    
    # å…ˆç”Ÿæˆè¾¹ç¼˜å›¾å’Œä¼ ç»Ÿé£æ ¼å€™é€‰
    edge_map = pipe._get_or_build_edge_map(ctx, ui_params)
    trad_candidate = pipe._get_or_build_traditional(ctx, ui_params)
    
    # æ ¹æ® use_diffusion å†³å®šæ˜¯å¦ç”Ÿæˆ Diffusion å€™é€‰
    if use_diffusion:
        try:
            candidates = pipe._get_or_build_candidates(ctx, edge_map, trad_candidate, ui_params)
            print("[Pipeline] Diffusion å€™é€‰ç”Ÿæˆå®Œæˆ")
        except Exception as e:
            print(f"[Pipeline] Diffusion ç”Ÿæˆå¤±è´¥ï¼Œé™çº§ä¸ºä¼ ç»Ÿæ–¹æ³•: {e}")
            candidates = {"Traditional": trad_candidate}
    else:
        candidates = {"Traditional": trad_candidate}
        print("[Pipeline] ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ï¼ˆDiffusion å·²å…³é—­ï¼‰")
    
    # æ›´æ–°çŠ¶æ€
    new_state.image_hash = img_hash
    new_state.ctx = ctx
    new_state.seg_out = seg_out
    new_state.face_mask = face_mask
    new_state.candidates = candidates
    new_state.trad_params = trad_params
    new_state.original_image = image.copy()
    new_state.active_masks = set()
    new_state.use_diffusion = use_diffusion
    
    print("[Pipeline] å®Œæ•´è®¡ç®—å®Œæˆï¼Œå·²ç¼“å­˜ä¸­é—´ç»“æœ")
    return new_state


def apply_tone_adjustment(
    image: np.ndarray,
    gamma: float,
    contrast: float,
    saturation: float,
    brightness: float
) -> np.ndarray:
    """åº”ç”¨è‰²è°ƒè°ƒæ•´ - Stage 3ï¼ˆæœ€è½»é‡ï¼‰"""
    import cv2
    
    result = image.copy()
    
    # Gamma
    if abs(gamma - 1.0) > 0.01:
        result = np.power(result, 1.0 / gamma)
    
    # Contrast
    if abs(contrast - 1.0) > 0.01:
        result = (result - 0.5) * contrast + 0.5
    
    # Brightness
    if abs(brightness) > 0.1:
        result = result + brightness / 255.0
    
    # Saturation
    if abs(saturation - 1.0) > 0.01:
        result_u8 = (np.clip(result, 0, 1) * 255).astype(np.uint8)
        hsv = cv2.cvtColor(result_u8, cv2.COLOR_RGB2HSV).astype(np.float32)
        hsv[:, :, 1] = np.clip(hsv[:, :, 1] * saturation, 0, 255)
        result_u8 = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)
        result = result_u8.astype(np.float32) / 255.0
    
    return np.clip(result, 0, 1).astype(np.float32)


def _unify_sizes(
    candidates: dict,
    seg_out,
    face_mask: np.ndarray | None,
    original_image: np.ndarray
) -> tuple[dict, dict, np.ndarray | None, np.ndarray]:
    """
    ç»Ÿä¸€æ‰€æœ‰å›¾åƒå’Œ mask çš„å°ºå¯¸
    
    ä»¥ç¬¬ä¸€ä¸ªå€™é€‰å›¾åƒçš„å°ºå¯¸ä¸ºåŸºå‡†ï¼Œè°ƒæ•´æ‰€æœ‰å…¶ä»–æ•°æ®çš„å°ºå¯¸ã€‚
    
    Args:
        candidates: é£æ ¼å€™é€‰å­—å…¸
        seg_out: åˆ†å‰²è¾“å‡º
        face_mask: äººè„¸é®ç½©
        original_image: åŸå›¾
    
    Returns:
        (candidates, semantic_masks, face_mask, original_image) - å°ºå¯¸ç»Ÿä¸€åçš„æ•°æ®
    """
    import cv2
    
    # è·å–ç›®æ ‡å°ºå¯¸ï¼ˆä»¥ç¬¬ä¸€ä¸ªå€™é€‰å›¾åƒä¸ºåŸºå‡†ï¼‰
    first_candidate = next(iter(candidates.values()))
    target_h, target_w = first_candidate.image.shape[:2]
    
    # è°ƒæ•´ semantic_masks
    unified_masks = {}
    for bucket_name, mask in seg_out.semantic_masks.items():
        if mask.shape[:2] != (target_h, target_w):
            mask = cv2.resize(mask, (target_w, target_h), interpolation=cv2.INTER_LINEAR)
        unified_masks[bucket_name] = mask
    
    # è°ƒæ•´ face_mask
    unified_face_mask = face_mask
    if face_mask is not None and face_mask.shape[:2] != (target_h, target_w):
        unified_face_mask = cv2.resize(face_mask, (target_w, target_h), interpolation=cv2.INTER_LINEAR)
    
    # è°ƒæ•´ original_image
    unified_original = original_image
    if original_image.shape[:2] != (target_h, target_w):
        unified_original = cv2.resize(original_image, (target_w, target_h), interpolation=cv2.INTER_LINEAR)
    
    return candidates, unified_masks, unified_face_mask, unified_original


def realtime_render(
    state: ProcessingState,
    ui_params: dict
) -> np.ndarray | None:
    """
    å®æ—¶æ¸²æŸ“ï¼ˆä¸é‡æ–°æ¨ç†ï¼‰- Stage 2 + Stage 3
    
    Args:
        state: ç”¨æˆ·ä¼šè¯çŠ¶æ€ï¼ˆåŒ…å«ç¼“å­˜çš„ä¸­é—´ç»“æœï¼‰
        ui_params: UI å‚æ•°å­—å…¸
    
    Returns:
        æ¸²æŸ“åçš„å›¾åƒ (uint8)
    """
    if not state.is_ready():
        return None
    
    pipe = get_pipeline()
    ctx = state.ctx
    seg_out = state.seg_out
    face_mask = state.face_mask
    candidates = state.candidates
    
    # ç»Ÿä¸€æ‰€æœ‰æ•°æ®çš„å°ºå¯¸ï¼ˆè§£å†³åˆ†å‰²è¾“å‡ºä¸é£æ ¼åŒ–å›¾åƒå°ºå¯¸ä¸åŒ¹é…çš„é—®é¢˜ï¼‰
    candidates, unified_masks, face_mask, original_image = _unify_sizes(
        candidates, seg_out, face_mask, ctx.image_f32
    )
    
    # åˆ›å»ºç»Ÿä¸€å°ºå¯¸åçš„ seg_out å‰¯æœ¬
    from src.context import SegmentationOutput
    unified_seg_out = SegmentationOutput(
        label_map=seg_out.label_map,  # label_map ä¸éœ€è¦åœ¨èåˆä¸­ä½¿ç”¨
        semantic_masks=unified_masks,
        seg_logits=seg_out.seg_logits
    )
    
    # D. è¯­ä¹‰è·¯ç”±ï¼ˆè½»é‡ï¼‰
    routing = pipe.router.route(
        semantic_masks=unified_seg_out.semantic_masks,
        face_mask=face_mask,
        ui_overrides=ui_params
    )
    
    # C2. åŒºåŸŸçº§é£æ ¼åŒ–ï¼ˆæŒ‰éœ€ç”Ÿæˆï¼Œå¸¦ç¼“å­˜ï¼‰
    region_candidates = pipe.region_stylizer.generate_region_styles(
        image_f32=original_image,
        image_hash=ctx.image_hash,
        seg_out=unified_seg_out,
        region_configs=routing.region_configs,
        global_candidates=candidates
    )
    
    # E. åŒºåŸŸèåˆï¼ˆè½»é‡ï¼‰
    fused = pipe.fuser.fuse(
        candidates=candidates,
        routing=routing,
        seg_out=unified_seg_out,
        method=ui_params.get("fusion_method", "soft_mask"),
        blur_kernel=ui_params.get("fusion_blur_kernel", 21),
        original_image=original_image,
        region_candidates=region_candidates
    )
    
    # F. å…¨å±€åè°ƒï¼ˆè½»é‡ï¼‰
    if ui_params.get("harmonization_enabled", True):
        ref = pipe.harmonizer.pick_reference(
            candidates, unified_seg_out, ui_params, pipe.cfg.harmonization
        )
        fused = pipe.harmonizer.match_and_adjust(fused, ref, ui_params)
    
    # G. çº¿ç¨¿å åŠ  - ä½¿ç”¨è¯­ä¹‰è·¯ç”±
    has_lineart = any(
        routing.region_configs.get(bucket, None) and 
        getattr(routing.region_configs.get(bucket), "lineart_strength", 0) > 0.01
        for bucket in unified_seg_out.semantic_masks.keys()
    )
    
    if has_lineart:
        fused = pipe.lineart.overlay_with_semantic_routing(
            image=fused,
            semantic_masks=unified_seg_out.semantic_masks,
            region_configs=routing.region_configs,
            params=ui_params
        )
    elif ui_params.get("edge_strength", 0) > 1e-3:
        edges = pipe.lineart.extract_from_stylized(fused, ui_params)
        fused = pipe.lineart.overlay(fused, edges, ui_params["edge_strength"], ui_params)
    
    # G2. ç»†èŠ‚å¢å¼º - ä½¿ç”¨è¯­ä¹‰è·¯ç”±
    has_detail = any(
        routing.region_configs.get(bucket, None) and 
        getattr(routing.region_configs.get(bucket), "detail_enhance", 0) > 0.01
        for bucket in unified_seg_out.semantic_masks.keys()
    )
    
    if has_detail:
        fused = pipe.lineart.enhance_detail_with_semantic_routing(
            image=fused,
            guide=original_image,
            semantic_masks=unified_seg_out.semantic_masks,
            region_configs=routing.region_configs
        )
    elif ui_params.get("detail_enhance_enabled", False):
        fused = pipe.lineart.enhance_detail(
            fused, original_image, ui_params.get("detail_strength", 0.5)
        )
    
    # Stage 3: è‰²è°ƒè°ƒæ•´ï¼ˆæœ€è½»é‡ï¼‰
    fused = apply_tone_adjustment(
        fused,
        ui_params.get("gamma", 1.0),
        ui_params.get("contrast", 1.0),
        ui_params.get("saturation", 1.0),
        ui_params.get("brightness", 0.0)
    )
    
    # åå¤„ç†
    out_u8 = pipe.preprocessor.postprocess(fused, ctx)
    return out_u8


def visualize_semantic_mask(
    state: ProcessingState,
    bucket: str,
    toggle: bool = True
) -> tuple[np.ndarray | None, str, ProcessingState]:
    """
    å¯è§†åŒ–æŒ‡å®šè¯­ä¹‰åŒºåŸŸçš„é®ç½©ï¼ˆæ”¯æŒå åŠ å¤šä¸ªåŒºåŸŸï¼‰
    
    Args:
        state: ç”¨æˆ·ä¼šè¯çŠ¶æ€
        bucket: è¯­ä¹‰æ¡¶åç§° æˆ– "FACE" æˆ– "NONE"
        toggle: æ˜¯å¦åˆ‡æ¢è¯¥åŒºåŸŸçš„æ˜¾ç¤ºçŠ¶æ€
    
    Returns:
        (å åŠ é®ç½©åçš„å›¾åƒ, è¦†ç›–ç‡ä¿¡æ¯, æ›´æ–°åçš„çŠ¶æ€)
    """
    import cv2
    
    if state.original_image is None or state.seg_out is None:
        return None, "è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†å›¾åƒ", state
    
    new_state = state.copy()
    
    # å¤„ç† NONEï¼ˆæ¸…é™¤æ‰€æœ‰é®ç½©ï¼‰
    if bucket == "NONE":
        new_state.active_masks = set()
        return state.original_image.copy(), "æ˜¾ç¤ºåŸå›¾", new_state
    
    # åˆ‡æ¢è¯¥åŒºåŸŸçš„æ¿€æ´»çŠ¶æ€
    if toggle:
        if bucket in new_state.active_masks:
            new_state.active_masks.discard(bucket)
        else:
            new_state.active_masks.add(bucket)
    
    # å¦‚æœæ²¡æœ‰æ¿€æ´»çš„é®ç½©ï¼Œè¿”å›åŸå›¾
    if not new_state.active_masks:
        return state.original_image.copy(), "ç‚¹å‡»åŒºåŸŸæŒ‰é’®æŸ¥çœ‹é®ç½©", new_state
    
    # è·å–åŸå›¾
    original = state.original_image.copy()
    H, W = original.shape[:2]
    result = original.astype(np.float32)
    
    info_parts = []
    
    # å åŠ æ‰€æœ‰æ¿€æ´»çš„é®ç½©
    for active_bucket in new_state.active_masks:
        # è·å–é®ç½©
        if active_bucket == "FACE":
            if state.face_mask is None:
                continue
            mask = state.face_mask
        else:
            if active_bucket not in state.seg_out.semantic_masks:
                continue
            mask = state.seg_out.semantic_masks[active_bucket]
        
        # è°ƒæ•´é®ç½©å°ºå¯¸åˆ°åŸå›¾å¤§å°
        if mask.shape[:2] != (H, W):
            mask = cv2.resize(mask, (W, H), interpolation=cv2.INTER_LINEAR)
        
        # è®¡ç®—è¦†ç›–ç‡
        coverage = mask.mean() * 100
        info_parts.append(f"{active_bucket}: {coverage:.1f}%")
        
        # åˆ›å»ºå½©è‰²é®ç½©
        color = SEMANTIC_COLORS.get(active_bucket, (255, 255, 0))
        colored_mask = np.zeros((H, W, 3), dtype=np.float32)
        colored_mask[:, :] = color
        
        # å åŠ é®ç½©ï¼ˆåŠé€æ˜ï¼‰
        alpha = 0.5
        mask_3d = np.stack([mask] * 3, axis=-1)
        result = result * (1 - mask_3d * alpha) + colored_mask * mask_3d * alpha
        
        # æ·»åŠ è¾¹ç•Œè½®å»“
        mask_u8 = (mask * 255).astype(np.uint8)
        contours, _ = cv2.findContours(mask_u8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        result_u8 = np.clip(result, 0, 255).astype(np.uint8)
        cv2.drawContours(result_u8, contours, -1, color, 2)
        result = result_u8.astype(np.float32)
    
    result = np.clip(result, 0, 255).astype(np.uint8)
    info = "ğŸ¯ " + " | ".join(info_parts) if info_parts else "æ— æ¿€æ´»åŒºåŸŸ"
    
    return result, info, new_state

